use anyhow::{Result, anyhow};
use dotenvy::dotenv;
use openai_api_rust::{
    Message, OpenAI,
    chat::{ChatApi, ChatBody},
};

#[derive(Default)]
/// Configuration options for LLM calls
pub struct LlmOptions {
    /// Model name (if None, will be read from environment variables)
    pub model: Option<String>,
    /// Temperature parameter (0.0-2.0)
    pub temperature: Option<f32>,
    /// Maximum number of tokens to generate
    pub max_tokens: Option<i32>,
    /// Whether to enable streaming output
    pub stream: bool,
}

/// Initialize the OpenAI client
///
/// Will read API key and other configurations from environment variables
pub fn init_openai() -> Result<OpenAI> {
    dotenv().ok();
    let openai = OpenAI::from_env().unwrap();
    Ok(openai)
}

/// Multi-turn dialogue LLM call
///
/// Suitable for multi-turn conversation scenarios like chat
///
/// # Arguments
/// * `messages` - Message history array
/// * `options` - Optional LLM configuration
///
/// # Returns
/// * Returns the text response generated by the LLM
pub fn call_llm_chat(messages: &[Message], options: Option<LlmOptions>) -> Result<String> {
    let options = options.unwrap_or_default();
    let openai = init_openai()?;
    let model = options.model.unwrap_or_else(|| {
        std::env::var("OPENAI_MODEL").unwrap_or_else(|_| "gpt-3.5-turbo".to_string())
    });

    let body = ChatBody {
        model,
        max_tokens: options.max_tokens,
        temperature: options.temperature,
        top_p: None,
        n: None,
        stream: Some(options.stream),
        stop: None,
        presence_penalty: None,
        frequency_penalty: None,
        logit_bias: None,
        user: None,
        messages: messages.to_vec(),
    };

    // Logic for handling streaming/non-streaming output is the same as call_llm_simple
    if options.stream {
        Err(anyhow!("Streaming not supported"))
    } else {
        let rs = openai.chat_completion_create(&body).unwrap();
        let choice = rs.choices;
        let message = &choice[0].message.as_ref().unwrap();
        Ok(message.content.clone())
    }
}
