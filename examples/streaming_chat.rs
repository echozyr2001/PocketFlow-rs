use pocketflow_rs::{
    Action, ExecutionContext, InMemoryStorage, SharedStore,
    node::NodeBackend,
    node::builtin::llm::{ApiConfig, ApiRequestNode},
};
use serde_json::json;
use std::io::{self, Write};
use std::time::Duration;
use tokio;

/// æµå¼èŠå¤©åº”ç”¨ç¤ºä¾‹
/// å±•ç¤ºå¦‚ä½•ä½¿ç”¨ PocketFlow-rs çš„æµå¼ API åŠŸèƒ½æ„å»ºå®æ—¶èŠå¤©ä½“éªŒ
#[tokio::main]
async fn main() -> Result<(), Box<dyn std::error::Error>> {
    println!("ğŸ¤– PocketFlow-rs æµå¼èŠå¤©åº”ç”¨");
    println!("=====================================");
    println!("è¯´æ˜ï¼šè¿™æ˜¯ä¸€ä¸ªæ¼”ç¤ºç¨‹åºï¼Œéœ€è¦è®¾ç½® OPENAI_API_KEY ç¯å¢ƒå˜é‡æ‰èƒ½æ­£å¸¸å·¥ä½œ");
    println!("è¾“å…¥ 'quit' é€€å‡ºï¼Œè¾“å…¥ 'stream on/off' åˆ‡æ¢æµå¼æ¨¡å¼");
    println!();

    // åˆå§‹åŒ–åº”ç”¨
    let mut chat_app = ChatApplication::new().await?;

    // å¼€å§‹èŠå¤©å¾ªç¯
    chat_app.run_chat_loop().await?;

    Ok(())
}

/// èŠå¤©åº”ç”¨ç»“æ„
pub struct ChatApplication {
    store: SharedStore<InMemoryStorage>,
    execution_context: ExecutionContext,
    streaming_node: ApiRequestNode,
    regular_node: ApiRequestNode,
    conversation_history: Vec<serde_json::Value>,
    use_streaming: bool,
}

impl ChatApplication {
    /// åˆ›å»ºæ–°çš„èŠå¤©åº”ç”¨
    pub async fn new() -> Result<Self, Box<dyn std::error::Error>> {
        let mut store: SharedStore<InMemoryStorage> = SharedStore::new();
        let execution_context = ExecutionContext::new(3, Duration::from_secs(30));

        // æ£€æŸ¥APIå¯†é’¥
        let api_key = std::env::var("OPENAI_API_KEY").unwrap_or_else(|_| "demo_key".to_string());

        if api_key == "demo_key" {
            println!("âš ï¸  æœªè®¾ç½® OPENAI_API_KEY ç¯å¢ƒå˜é‡ï¼Œå°†ä½¿ç”¨æ¼”ç¤ºæ¨¡å¼");
            println!("   å®é™…APIè°ƒç”¨å°†å¤±è´¥ï¼Œä½†å¯ä»¥æŸ¥çœ‹æµç¨‹æ¼”ç¤º");
            println!();
        }

        // åˆ›å»ºæµå¼APIé…ç½®
        let streaming_config = ApiConfig {
            api_key: api_key.clone(),
            base_url: None,
            org_id: None,
            model: "gpt-3.5-turbo".to_string(),
            max_tokens: Some(1000),
            temperature: Some(0.7),
            top_p: None,
            frequency_penalty: None,
            presence_penalty: None,
            timeout: Some(30),
            stream: true, // å¯ç”¨æµå¼å“åº”
        };

        // åˆ›å»ºå¸¸è§„APIé…ç½®
        let regular_config = ApiConfig {
            stream: false, // ç¦ç”¨æµå¼å“åº”
            ..streaming_config.clone()
        };

        // åˆ›å»ºAPIèŠ‚ç‚¹
        let streaming_node =
            ApiRequestNode::new("messages", "response", Action::simple("continue"))
                .with_config(streaming_config)
                .with_system_message("ä½ æ˜¯ä¸€ä¸ªæœ‰å¸®åŠ©çš„AIåŠ©æ‰‹ã€‚è¯·å‹å¥½ã€å‡†ç¡®åœ°å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚");

        let regular_node = ApiRequestNode::new("messages", "response", Action::simple("continue"))
            .with_config(regular_config)
            .with_system_message("ä½ æ˜¯ä¸€ä¸ªæœ‰å¸®åŠ©çš„AIåŠ©æ‰‹ã€‚è¯·å‹å¥½ã€å‡†ç¡®åœ°å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚");

        Ok(Self {
            store,
            execution_context,
            streaming_node,
            regular_node,
            conversation_history: Vec::new(),
            use_streaming: true, // é»˜è®¤å¯ç”¨æµå¼æ¨¡å¼
        })
    }

    /// è¿è¡ŒèŠå¤©å¾ªç¯
    pub async fn run_chat_loop(&mut self) -> Result<(), Box<dyn std::error::Error>> {
        loop {
            // æ˜¾ç¤ºå½“å‰æ¨¡å¼
            let mode_indicator = if self.use_streaming {
                "ğŸ”„ æµå¼"
            } else {
                "ğŸ“¦ å¸¸è§„"
            };
            print!("{} æ¨¡å¼ > ", mode_indicator);
            io::stdout().flush()?;

            // è¯»å–ç”¨æˆ·è¾“å…¥
            let mut input = String::new();
            io::stdin().read_line(&mut input)?;
            let user_input = input.trim();

            // å¤„ç†ç‰¹æ®Šå‘½ä»¤
            match user_input {
                "quit" | "exit" => {
                    println!("ğŸ‘‹ å†è§ï¼");
                    break;
                }
                "stream on" => {
                    self.use_streaming = true;
                    println!("âœ… å·²åˆ‡æ¢åˆ°æµå¼æ¨¡å¼");
                    continue;
                }
                "stream off" => {
                    self.use_streaming = false;
                    println!("âœ… å·²åˆ‡æ¢åˆ°å¸¸è§„æ¨¡å¼");
                    continue;
                }
                "clear" => {
                    self.conversation_history.clear();
                    println!("ğŸ—‘ï¸ å¯¹è¯å†å²å·²æ¸…é™¤");
                    continue;
                }
                "help" => {
                    self.show_help();
                    continue;
                }
                _ if user_input.is_empty() => continue,
                _ => {}
            }

            // å¤„ç†èŠå¤©æ¶ˆæ¯
            if let Err(e) = self.process_chat_message(user_input).await {
                println!("âŒ å¤„ç†æ¶ˆæ¯æ—¶å‡ºé”™: {}", e);
            }

            println!(); // æ·»åŠ ç©ºè¡Œåˆ†éš”
        }

        Ok(())
    }

    /// å¤„ç†èŠå¤©æ¶ˆæ¯
    async fn process_chat_message(
        &mut self,
        user_input: &str,
    ) -> Result<(), Box<dyn std::error::Error>> {
        // æ·»åŠ ç”¨æˆ·æ¶ˆæ¯åˆ°å†å²
        let user_message = json!({
            "role": "user",
            "content": user_input
        });
        self.conversation_history.push(user_message);

        // å‡†å¤‡æ¶ˆæ¯æ•°ç»„
        let messages = json!(self.conversation_history);
        self.store.set("messages".to_string(), messages)?;

        // é€‰æ‹©ä½¿ç”¨çš„èŠ‚ç‚¹
        let node = if self.use_streaming {
            &mut self.streaming_node
        } else {
            &mut self.regular_node
        };

        println!();
        print!("ğŸ¤– AIåŠ©æ‰‹: ");
        io::stdout().flush()?;

        let start_time = std::time::Instant::now();

        // æ‰§è¡ŒAPIè°ƒç”¨
        match node.prep(&self.store, &self.execution_context).await {
            Ok(prepared_messages) => {
                // æ˜¾ç¤ºæ­£åœ¨å¤„ç†çš„æ¶ˆæ¯æ•°é‡
                if self.use_streaming {
                    println!("(æµå¼å“åº”ä¸­...)");
                } else {
                    print!("(å¤„ç†ä¸­...)");
                    io::stdout().flush()?;
                }

                // æ‰§è¡ŒAPIè¯·æ±‚
                match <ApiRequestNode as NodeBackend<InMemoryStorage>>::exec(
                    node,
                    prepared_messages,
                    &self.execution_context,
                )
                .await
                {
                    Ok(response) => {
                        let duration = start_time.elapsed();

                        if !self.use_streaming {
                            // éæµå¼æ¨¡å¼ï¼šä¸€æ¬¡æ€§æ˜¾ç¤ºå®Œæ•´å“åº”
                            println!("\rğŸ¤– AIåŠ©æ‰‹: {}", response);
                        } else {
                            // æµå¼æ¨¡å¼ï¼šå“åº”å·²ç»åœ¨æµå¼å¤„ç†ä¸­æ˜¾ç¤º
                            println!("{}", response);
                        }

                        println!("â±ï¸  å“åº”æ—¶é—´: {:.2?}", duration);

                        // æ·»åŠ AIå“åº”åˆ°å†å²
                        let assistant_message = json!({
                            "role": "assistant",
                            "content": response
                        });
                        self.conversation_history.push(assistant_message);

                        // é™åˆ¶å†å²é•¿åº¦ï¼ˆä¿ç•™æœ€è¿‘20æ¡æ¶ˆæ¯ï¼‰
                        if self.conversation_history.len() > 20 {
                            self.conversation_history.drain(0..2); // ç§»é™¤æœ€æ—©çš„ä¸€è½®å¯¹è¯
                        }
                    }
                    Err(e) => {
                        let duration = start_time.elapsed();
                        println!("âŒ APIè°ƒç”¨å¤±è´¥: {}", e);

                        if e.to_string().contains("demo_key") || e.to_string().contains("auth") {
                            println!("ğŸ’¡ æç¤º: è¯·è®¾ç½®æœ‰æ•ˆçš„ OPENAI_API_KEY ç¯å¢ƒå˜é‡");
                            println!("   export OPENAI_API_KEY=your_api_key_here");
                        }

                        println!("â±ï¸  å¤±è´¥æ—¶é—´: {:.2?}", duration);
                    }
                }
            }
            Err(e) => {
                println!("âŒ æ¶ˆæ¯å‡†å¤‡å¤±è´¥: {}", e);
            }
        }

        Ok(())
    }

    /// æ˜¾ç¤ºå¸®åŠ©ä¿¡æ¯
    fn show_help(&self) {
        println!();
        println!("ğŸ“– å¯ç”¨å‘½ä»¤:");
        println!("  help        - æ˜¾ç¤ºæ­¤å¸®åŠ©ä¿¡æ¯");
        println!("  stream on   - å¯ç”¨æµå¼æ¨¡å¼ï¼ˆå®æ—¶æ˜¾ç¤ºå“åº”ï¼‰");
        println!("  stream off  - å¯ç”¨å¸¸è§„æ¨¡å¼ï¼ˆå®Œæ•´å“åº”ä¸€æ¬¡æ˜¾ç¤ºï¼‰");
        println!("  clear       - æ¸…é™¤å¯¹è¯å†å²");
        println!("  quit/exit   - é€€å‡ºåº”ç”¨");
        println!();
        println!("ğŸ’¡ æç¤º:");
        println!("  - æµå¼æ¨¡å¼æä¾›å®æ—¶å“åº”ä½“éªŒï¼Œé€‚åˆé•¿å¯¹è¯");
        println!("  - å¸¸è§„æ¨¡å¼ç­‰å¾…å®Œæ•´å“åº”åæ˜¾ç¤ºï¼Œé€‚åˆçŸ­æŸ¥è¯¢");
        println!("  - å¯¹è¯å†å²è‡ªåŠ¨ä¿æŒï¼Œæ”¯æŒä¸Šä¸‹æ–‡å¯¹è¯");
        println!();
    }
}

/// æ¼”ç¤ºæ‰¹é‡æµ‹è¯•åŠŸèƒ½
#[allow(dead_code)]
async fn demo_batch_processing() -> Result<(), Box<dyn std::error::Error>> {
    println!("ğŸ”¬ æ¼”ç¤ºæ‰¹é‡å¤„ç†åŠŸèƒ½");

    let mut store: SharedStore<InMemoryStorage> = SharedStore::new();
    let execution_context = ExecutionContext::new(3, Duration::from_secs(30));

    // åˆ›å»ºæµ‹è¯•é…ç½®
    let config = ApiConfig {
        api_key: "demo_key".to_string(),
        base_url: None,
        org_id: None,
        model: "gpt-3.5-turbo".to_string(),
        max_tokens: Some(100),
        temperature: Some(0.7),
        top_p: None,
        frequency_penalty: None,
        presence_penalty: None,
        timeout: Some(10),
        stream: true,
    };

    let mut node =
        ApiRequestNode::new("input", "output", Action::simple("next")).with_config(config);

    // æµ‹è¯•ä¸åŒç±»å‹çš„æ¶ˆæ¯
    let test_messages = vec![
        json!("ä½ å¥½ï¼Œè¯·ç®€å•ä»‹ç»ä¸€ä¸‹äººå·¥æ™ºèƒ½"),
        json!([
            {"role": "user", "content": "ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Ÿ"},
            {"role": "assistant", "content": "æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªé‡è¦åˆ†æ”¯..."},
            {"role": "user", "content": "è¯·ä¸¾ä¸ªå®é™…åº”ç”¨çš„ä¾‹å­"}
        ]),
        json!("è§£é‡Šä¸€ä¸‹é‡å­è®¡ç®—çš„åŸºæœ¬æ¦‚å¿µ"),
    ];

    for (i, test_input) in test_messages.into_iter().enumerate() {
        println!(
            "\nğŸ“ æµ‹è¯• {} - è¾“å…¥ç±»å‹: {}",
            i + 1,
            if test_input.is_string() {
                "ç®€å•æ–‡æœ¬"
            } else {
                "å¯¹è¯å†å²"
            }
        );

        store.set("input".to_string(), test_input)?;

        match node.prep(&store, &execution_context).await {
            Ok(messages) => {
                println!("âœ… æ¶ˆæ¯å‡†å¤‡æˆåŠŸ: {} æ¡æ¶ˆæ¯", messages.len());
                // æ³¨æ„: è¿™é‡Œä¼šå› ä¸ºæ²¡æœ‰æœ‰æ•ˆAPIå¯†é’¥è€Œå¤±è´¥ï¼Œä½†å±•ç¤ºäº†æµç¨‹
            }
            Err(e) => {
                println!("âŒ æ¶ˆæ¯å‡†å¤‡å¤±è´¥: {}", e);
            }
        }
    }

    Ok(())
}

/// æ¼”ç¤ºé…ç½®æ¯”è¾ƒ
#[allow(dead_code)]
fn demo_config_comparison() {
    println!("âš™ï¸  é…ç½®å¯¹æ¯”æ¼”ç¤º");

    // æµå¼é…ç½®
    let streaming_config = ApiConfig::default()
        .with_model("gpt-4".to_string())
        .with_stream(true)
        .with_max_tokens(2000)
        .with_temperature(0.8);

    // å¸¸è§„é…ç½®
    let regular_config = ApiConfig::default()
        .with_model("gpt-3.5-turbo".to_string())
        .with_stream(false)
        .with_max_tokens(1000)
        .with_temperature(0.7);

    println!("ğŸ”„ æµå¼é…ç½®:");
    println!("  æ¨¡å‹: {}", streaming_config.model);
    println!("  æµå¼: {}", streaming_config.stream);
    println!("  æœ€å¤§ä»¤ç‰Œ: {:?}", streaming_config.max_tokens);
    println!("  æ¸©åº¦: {:?}", streaming_config.temperature);

    println!("\nğŸ“¦ å¸¸è§„é…ç½®:");
    println!("  æ¨¡å‹: {}", regular_config.model);
    println!("  æµå¼: {}", regular_config.stream);
    println!("  æœ€å¤§ä»¤ç‰Œ: {:?}", regular_config.max_tokens);
    println!("  æ¸©åº¦: {:?}", regular_config.temperature);
}
